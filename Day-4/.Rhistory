hulls <- ddply(iseng2, "cluster", find_hull)
hulls <- ddply(iseng2, "clusters", find_hull)
repeat {
result <- tryCatch({
peringatan = ""
peta = get_map(location = "Riau", zoom = 7)
}, warning = function(w) {
peringatan = "warning"
print(peringatan)
}, error = function(e) {
peringatan = "error"
print(peringatan)
})
if (result != "warning" && result != "error") {
print("map successfully loaded")
break
}
}
ggmap(mapFromGoogle()) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE, size = BRIGHTNESS, color = clusters)) +
geom_polygon(data=hulls,fill=NA)
ggmap(result) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE, size = BRIGHTNESS, color = clusters)) +
geom_polygon(data=hulls,fill=NA)
ggmap(result) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE, size = BRIGHTNESS, color = clusters)) +
geom_polygon(data=hulls,fill=NA)
ggmap(result) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE, size = BRIGHTNESS, color = clusters)) +
geom_polygon(data=hulls,fill=NA, inherit.aes = FALSE)
ggmap(result) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE, color = clusters)) +
geom_polygon(data=hulls,fill=NA, inherit.aes = FALSE)
ggmap(result) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE, color = clusters)) +
geom_point() +
geom_polygon(data=hulls,fill=NA, inherit.aes = FALSE)
hulls
iseng2
ggmap(result) +
geom_polygon(data = iseng2, aes(x = LONGITUDE, y = LATITUDE, color = clusters)) +
geom_point()
ggmap(result) +
geom_polygon(data = hulls, aes(x = LONGITUDE, y = LATITUDE, color = clusters)) +
geom_point()
ggmap(result) +
geom_point(data = iseng2)
ggmap(result) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE))
ggmap(result) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE)) +
geom_polygon(data = hulls, aes(x = LONGITUDE, y = LATITUDE, color = clusters)) +
ggplot(iseng2,aes(x=LONGITUDE,y=LATITUDE,color=cluster))+geom_point()+
geom_polygon(data=hulls,fill=NA)+ theme_bw()
ggmap(result) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE)) +
geom_polygon(data = hulls, aes(x = LONGITUDE, y = LATITUDE, color = clusters))
ggmap(result) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE)) +
geom_polygon(data = hulls, fill = NA)
ggmap(result) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE, color = clusters)) +
geom_polygon(data = hulls, aes(x = LONGITUDE, y = LATITUDE), fill = NA)
iseng
hulls
ggmap(result) +
geom_point(data = iseng2, aes(x = LONGITUDE, y = LATITUDE, color = clusters)) +
geom_polygon(data = hulls, aes(x = LONGITUDE, y = LATITUDE, color = clusters), fill = NA)
runApp('dbscan-app')
setwd("E://Kerjaan/Kerjaan Statistik/DBSCAN/dbscan-web-app/")
df_1 = read.csv2(file = "Data-Riau.csv")
iseng = df_1[df_1$TAHUN = 2017, ]
iseng = df_1[df_1$TAHUN = 2017, ]
iseng = df_1[df_1$TAHUN == 2017, ]
iseng = df_1[df_1$TAHUN == 2017, -1]
hasil = dbscan::dbscan(distm(x = iseng[,1:2], fun = distHaversine), eps = 100*1000,minPts = 4)
iseng = df_1[df_1$TAHUN == 2017 & df_1$BULAN == 1, -1]
hasil = dbscan::dbscan(distm(x = iseng[,1:2], fun = distHaversine), eps = 100*1000,minPts = 4)
iseng$clusters = as.factor(hasil$cluster)
library(spatstat)
setwd("E:/Latihan/latihan-R/100DaysOfMLCode/Day-4/")
df = read.csv2(file = "dataR2.csv",header = TRUE, stringsAsFactors = FALSE)
df
df = read.csv2(file = "dataR2.csv",header = TRUE, stringsAsFactors = FALSE, sep = ',')
df
# Take data from internet (UCI Datasets)
download = getURL("https://archive.ics.uci.edu/ml/machine-learning-databases/00451/dataR2.csv")
# Take data from internet (UCI Datasets)
download = getURL("https://archive.ics.uci.edu/ml/machine-learning-databases/00451/dataR2.csv")
# Take data from internet (UCI Datasets)
download = getURL("https://archive.ics.uci.edu/ml/machine-learning-databases/00451/dataR2.csv")
# Take data from internet (UCI Datasets)
download = read.csv2(url = "https://raw.githubusercontent.com/hadimaster65555/100DaysOfMLCode-ShinyKmeansHotspotAnalysis/master/Data-Riau.csv")
# Take data from internet (UCI Datasets)
download = read.csv("https://raw.githubusercontent.com/hadimaster65555/100DaysOfMLCode-ShinyKmeansHotspotAnalysis/master/Data-Riau.csv")
# Take data from internet (UCI Datasets)
download = read.csv("https://raw.githubusercontent.com/hadimaster65555/100DaysOfMLCode-ShinyKmeansHotspotAnalysis/master/Data-Riau.csv", sep = ',')
# Take data from internet (UCI Datasets)
download = read.csv("https://raw.githubusercontent.com/hadimaster65555/100DaysOfMLCode-ShinyKmeansHotspotAnalysis/master/Data-Riau.csv", header = FALSE, sep = ',')
download
# Take data from internet (UCI Datasets)
download = read.csv("https://raw.githubusercontent.com/hadimaster65555/100DaysOfMLCode-ShinyKmeansHotspotAnalysis/master/Data-Riau.csv", header = TRUE, sep = ';')
download
# Take data from internet (UCI Datasets)
download = read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00451/dataR2.csv", header = TRUE, sep = ',')
download
# Take data from internet (UCI Datasets)
df = read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00451/dataR2.csv", header = TRUE, sep = ',')
df
# Check datatype
str(df)
# Classification
plot(x = df$Age, y = df$BMI)
# Classification
plot(x = Age, y = BMI, data = df)
# Classification
plot(x = Age, y = BMI, data = df)
# Classification
plot(df[,1:2])
# Classification
cor(df)
# Classification
plot(cor(df))
library(corrplot)
# Plot
corrMatrix = cor(df)
corrplot(corrMatrix, method = "ellipse")
corrplot(corrMatrix, method = "square")
corrMatrix
corrMatrix
# Check datatype
str(df)
# Take data from internet (UCI Datasets)
df = read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00451/dataR2.csv", header = TRUE, sep = ',', stringsAsFactors = TRUE)
# Check datatype
str(df)
# Change Classification datatype to factor
df$Classification = as.factor(df$Classification)
# Check datatype
str(df)
# Correlation Plot
corrMatrix = cor(df)
# Correlation Plot
corrMatrix = cor(df)
corrplot(corrMatrix, method = "square")
# Correlation Plot
corrMatrix = cor(df)
df
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.6, replace = FALSE)
index
training_data = df[i,]
training_data = df[index,]
testData = df[-index,]
test_Data = df[-index,]
rm(testData)
test_Data = df[-index,]
test_Data
library(randomForest)
# Search importance variabel using random forest
rf_result = randomForest(Classification ~. , data = df)
importance(rf_result)
order(importance(rf_result))
sort(importance(rf_result))
sort(importance(rf_result))
sort(importance(rf_result), decreasing = FALSE)
sort(importance(rf_result), decreasing = TRUE)
imp = importance(rf_result)
imp
sort(imp, decreasing = TRUE)
str(imp)
summary(imp)
importance(rf_result)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.6, replace = FALSE)
training_data = df[index,]
test_Data = df[-index,]
# Check class bias
table(df$Classification)
# Check datatype
str(df)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.6, replace = FALSE)
training_data = df[index, c(1,3,8)]
test_Data = df[-index, c(1,3,8)]
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.6, replace = FALSE)
training_data = df[index, c(1,3,8,10)]
test_Data = df[-index, c(1,3,8,10)]
test_Data
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
predicted
logitMod
test_Data
predicted = predict(logitMod, test_Data, type = "response")
predicted
install.packages("InformationValue")
library(InformationValue)
misClassError(test_Data$Classification, predicted, threshold = "optCutOff")
predicted
predicted = plogis(predict(logitMod, test_Data))
predicted
plogis
predicted = plogis(predict(logitMod, test_Data))
predicted
misClassError(test_Data$Classification, predicted, threshold = "optCutOff")
misClassError(test_Data$Classification, predicted, threshold = "optCutOff")
predicted = plogis(predict(logitMod, test_Data))
predicted
test_Data$Classification
predicted
length(predicted)
test_Data$Classification
optCutOff <- optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = "optCutOff")
optCutOff
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.6, replace = FALSE)
training_data = df[index, c(1,3,8,10)]
test_Data = df[-index, c(1,3,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = plogis(predict(logitMod, test_Data))
optCutOff <- optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Search importance variabel using random forest
rf_result = randomForest(Classification ~. , data = df)
importance(rf_result)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.6, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
importance(rf_result)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.75, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.7, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Search importance variabel using random forest
rf_result = randomForest(Classification ~. , data = df)
importance(rf_result)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.7, replace = FALSE)
training_data = df[index, c(1,3,8,10)]
test_Data = df[-index, c(1,3,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.7, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.7, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.7, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.7, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.7, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.7, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.7, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
nrow(df)*0.75
nrow(df)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.65, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.65, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.65, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.65, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
test_Data
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.65, replace = FALSE)
training_data = df[index, c(1,8,10)]
test_Data = df[-index, c(1,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
importance(rf_result)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.65, replace = FALSE)
training_data = df[index, c(1,2,3,8,10)]
test_Data = df[-index, c(1,2,3,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.65, replace = FALSE)
training_data = df[index, c(1,2,3,8,10)]
test_Data = df[-index, c(1,2,3,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
predicted = predict(logitMod, test_Data, type = "response")
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
misClassError(test_Data$Classification, predicted, threshold = optCutOff)
library(caret)
install.packages("survival")
library(caret)
library("e1071")
# Using SVM
svmMod = svm(Classification ~ ., data = training_data)
summary(logitMod)
# Find miss classification error
missLogit = misClassError(test_Data$Classification, predicted, threshold = optCutOff)
missLogit
# Using SVM model to predict new value
predicted = predict(svmMod, test_Data, type = "response")
predicted
misClassError(test_Data$Classification, predicted)
optimalCutoff(actuals = test_Data$Classification, predictedScores = predicted)
optimalCutoff(actuals = test_Data$Classification, predictedScores = predicted)[1]
optimalCutoff(test_Data$Classification, predicted)[1]
optimalCutoff(test_Data$Classification, predicted)[1]
test_Data$Classification
length(test_Data$Classification)
length(predicted)
predicted
test_Data$Classification
test_Data$Classification
optimalCutoff(test_Data$Classification, predicted)[1]
table(prediced, test_Data$Classification)
table(predicted, test_Data$Classification)
misClassError(test_Data$Classification, predicted)
misClassError(as.numeric(test_Data$Classification), predicted)
str(predicted)
str(test_Data$Classification)
# Find predicted value based on logitModel
predicted = predict(logitMod, test_Data, type = "response")
str(predicted)
# Using SVM model to predict new value
predicted = predict(svmMod, test_Data, type = "response")
table(predicted, test_Data$Classification)
misClassError(test_Data$Classification, predicted)
mean(predicted == test_Data$Classiication)
str(predicted)
mean(predicted == test_Data$Classiication)
predicted == test_Data$Classiication
predicted
as.numeric(predicted)
as.numeric(predicted) == test_Data$Classiication
as.numeric(predicted) == test_Data$Classification
mean(as.numeric(predicted) == test_Data$Classification)
mean(predicted == test_Data$Classification)
predicted == test_Data$Classification
trueVal = predicted == test_Data$Classification
trueVal
trueVal[trueVal == TRUE]
length(trueVal[trueVal == TRUE])/length(trueVal)
predicted
plot(svmMod)
plot(svmMod, data = training_data)
svmMod$terms
plot(svmMod$terms, data = training_data)
svmMod$labels
plot(svmMod$, data = training_data)
# Accuracy
mean(predicted == test_Data$Classification)
1-missLogit
# Split data to training and test data
index = sample(x = 1:nrow(df), size = nrow(df)*0.65, replace = FALSE)
training_data = df[index, c(1,2,3,8,10)]
test_Data = df[-index, c(1,2,3,8,10)]
# Using logistic regression
logitMod = glm(Classification ~., data = training_data, family = binomial(link = "logit"))
summary(logitMod)
# Find predicted value based on logitModel
predicted = predict(logitMod, test_Data, type = "response")
# Find Optimal Cutoff
optCutOff = optimalCutoff(test_Data$Classification, predicted)[1]
# Find miss classification error
missLogit = misClassError(test_Data$Classification, predicted, threshold = optCutOff)
missLogit
# Accuracy
logictAcc = 1 - missLogit
# Using SVM
svmMod = svm(Classification ~ ., data = training_data)
# Using SVM model to predict new value
predicted = predict(svmMod, test_Data, type = "response")
# Confussion table
table(predicted, test_Data$Classification)
# Accuracy
mean(predicted == test_Data$Classification)
# Accuracy
svmAcc = mean(predicted == test_Data$Classification)
# Summary
data.frame(Method = c("Logistic Regression", "Support Vector Machine"), Accuracy = c(logictAcc, svmAcc))
